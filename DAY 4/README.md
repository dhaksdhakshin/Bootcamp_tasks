# Regression Model Tasks – Student Review

## Task 1: Basic Linear Regression

**What I Learned:**
- How linear regression works with one feature.
- How to fit a model and interpret predictions.
- R-squared helps measure how well the model fits the data.

**Difficulties Faced:**
- Understanding what R-squared actually means was a bit tricky at first.
- Needed to practice data preparation before training the model.

**Difficulty Level:** Easy

---

## Task 2: Data Visualization for Regression

**What I Learned:**
- How to plot data points and the regression line.
- Visualization makes it easier to understand the trend.

**Difficulties Faced:**
- At first, I struggled with Matplotlib and Seaborn syntax.
- Had to learn how to properly label axes and add a trendline.

**Difficulty Level:** Moderate

---

## Task 3: Multiple Linear Regression

**What I Learned:**
- Regression with multiple predictors (features).
- The importance of feature scaling for improving performance.
- How correlated features impact predictions.

**Difficulties Faced:**
- Handling more than one feature made me confused about which features were most important.
- Needed more practice on interpreting coefficients.

**Difficulty Level:** Challenging

---

## Task 4: Model Assessment

**What I Learned:**
- How to evaluate models using R-squared and RMSE.
- RMSE helps understand how far predictions are from actual values.

**Difficulties Faced:**
- At first, I misunderstood the difference between RMSE and R-squared.
- Needed time to learn how to improve model performance.

**Difficulty Level:** Difficult

---

## Task 5: Feature Impact Analysis

**What I Learned:**
- How to check which features affect predictions the most.
- Feature importance and coefficient analysis.

**Difficulties Faced:**
- Struggled with interpreting coefficients properly.
- Had to research how to normalize features before comparing their impact.

**Difficulty Level:** Very Difficult

---

## Task 6: Polynomial Regression

**What I Learned:**
- How polynomial regression helps with non-linear relationships.
- Overfitting happens when the degree is too high.

**Difficulties Faced:**
- Needed time to understand how higher-degree polynomials affect the model.
- Visualization helped, but choosing the right degree was hard.

**Difficulty Level:** Very Difficult

---

## Task 7: Outlier Impact

**What I Learned:**
- Outliers affect regression models significantly.
- Removing outliers can improve model accuracy.

**Difficulties Faced:**
- Finding which outliers to remove was confusing.
- Needed more knowledge on box plots and z-score techniques.

**Difficulty Level:** Very Difficult

---

## Task 8: Regularization Implementation (Lasso & Ridge)

**What I Learned:**
- Lasso shrinks some coefficients to zero (feature selection).
- Ridge reduces large coefficients without removing them.
- Regularization prevents overfitting.

**Difficulties Faced:**
- Choosing the best alpha value was tricky.
- Understanding the impact of regularization on coefficients needed practice.

**Difficulty Level:** Very Difficult

---

## Bonus: Real-World Application – House Price Prediction

**What I Learned:**
- How to build a complete regression model with real-world context.
- Importance of feature engineering and data cleaning.
- How to apply all previous tasks in a real dataset.

**Difficulties Faced:**
- Finding real datasets with clean data was challenging.
- Debugging issues with feature selection and performance.
- Needed trial and error to improve model accuracy.

**Difficulty Level:** Extremely Challenging

---
